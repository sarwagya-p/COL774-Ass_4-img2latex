{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, layers, hparams):\n",
    "        '''\n",
    "        Args:\n",
    "            layers: Description of all layers in the Encoder: [(layer_type, {layer_params})]\n",
    "                - layer types - ['conv1d', 'conv2d', 'maxpool1d', 'maxpool2d', 'avgpool2d', 'avgpool2d', 'linear', 'dropout']\n",
    "                - layer_params - dict of parameters for the layer\n",
    "\n",
    "            hparams: Hyperparameters for the model\n",
    "        '''\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.hp = hparams\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for layer_type, layer_params in layers:\n",
    "            if layer_type == 'conv1d':\n",
    "                self.layers.append(nn.Conv1d(**layer_params))\n",
    "            elif layer_type == 'conv2d':\n",
    "                self.layers.append(nn.Conv2d(**layer_params))\n",
    "            elif layer_type == 'maxpool1d':\n",
    "                self.layers.append(nn.MaxPool1d(**layer_params))\n",
    "            elif layer_type == 'maxpool2d':\n",
    "                self.layers.append(nn.MaxPool2d(**layer_params))\n",
    "            elif layer_type == 'avgpool1d':\n",
    "                self.layers.append(nn.AvgPool1d(**layer_params))\n",
    "            elif layer_type == 'avgpool2d':\n",
    "                self.layers.append(nn.AvgPool2d(**layer_params))\n",
    "            elif layer_type == 'linear':\n",
    "                self.layers.append(nn.Linear(**layer_params))\n",
    "            elif layer_type == 'dropout':\n",
    "                self.layers.append(nn.Dropout(**layer_params))\n",
    "            else:\n",
    "                raise ValueError(f'Invalid layer type: {layer_type}')\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer(input)\n",
    "        return input\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab, vocab_dict, input_size, embedding_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            vocabulary_size: Size of the vocabulary\n",
    "            embedding_size: Size of the embedding vector\n",
    "        '''\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.vocab_dict = vocab_dict\n",
    "\n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_size)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.lstm = nn.LSTM(input_size+embedding_size, embedding_size, batch_first=True)\n",
    "        self.output = nn.Linear(embedding_size, len(vocab))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        '''\n",
    "        Args:\n",
    "            input: Input to the decoder\n",
    "            hidden: Hidden state of the previous time step\n",
    "        '''\n",
    "        # prev_embed = self.embedding(prev_tokens)\n",
    "        # concated_inp = torch.cat((input, prev_embed), dim=1)\n",
    "        if hidden is None:\n",
    "            output, hidden = self.lstm(input)\n",
    "        else:\n",
    "            output, hidden = self.lstm(input, hidden)\n",
    "        output = self.output(output)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "PAD = \"<pad>\"\n",
    "SOS = \"<sos>\"\n",
    "EOS = \"<eos>\"\n",
    "\n",
    "def load_img(path, size = (224, 224)):\n",
    "    img = (Image.open(path))\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Resize(size, antialias=True), transforms.Normalize(0, 255)])\n",
    "    return transform(img).detach()\n",
    "\n",
    "class Img2LatexDataset(data.Dataset):\n",
    "    def __init__(self, img_dir, formula_path, img_size = (224, 224)):\n",
    "        self.data_frame = pd.read_csv(formula_path)\n",
    "        self.img_dir = img_dir\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.token_to_idx = {}\n",
    "        self.tokens = []\n",
    "\n",
    "        for row in self.data_frame[\"formula\"]:\n",
    "            row = row.split()\n",
    "\n",
    "            for token in row:\n",
    "                if token not in self.token_to_idx:\n",
    "                    self.token_to_idx[token] = len(self.token_to_idx)\n",
    "                    self.tokens.append(token)\n",
    "        \n",
    "        for special_token in [SOS, EOS, PAD]:\n",
    "            self.token_to_idx[special_token] = len(self.token_to_idx)\n",
    "            self.tokens.append(special_token)\n",
    "\n",
    "        max_len = max([len(row.split()) for row in self.data_frame[\"formula\"]])+2\n",
    "        def indexer(row):\n",
    "            index_list = [self.token_to_idx[SOS]]\n",
    "            index_list.extend([self.token_to_idx[token] for token in row.split()])\n",
    "            index_list.append(self.token_to_idx[EOS])\n",
    "            index_list.extend([self.token_to_idx[PAD]] * (max_len - len(index_list)))\n",
    "\n",
    "            return index_list\n",
    "        \n",
    "        self.data_frame[\"IndexList\"] = self.data_frame[\"formula\"].apply(indexer)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = load_img(self.img_dir + self.data_frame[\"image\"][index], self.img_size)\n",
    "        return img, torch.tensor(self.data_frame[\"IndexList\"][index], requires_grad=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.token_to_idx, self.tokens\n",
    "\n",
    "img_dir = \"../data/SyntheticData/images/\"\n",
    "formula_dir = \"../data/SyntheticData/train.csv\"\n",
    "\n",
    "dataset = Img2LatexDataset(img_dir, formula_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"lr\" : 0.01,\n",
    "    \"batch_size\" : 64,\n",
    "    \"epochs\" : 10\n",
    "}\n",
    "\n",
    "channel_seq = [3, 32, 64, 128, 256, 512]\n",
    "num_conv_pool = 5\n",
    "\n",
    "enc_layers = []\n",
    "\n",
    "for i in range(num_conv_pool):\n",
    "    enc_layers.append(('conv2d', {'in_channels': channel_seq[i], 'out_channels': channel_seq[i+1], 'kernel_size': 5}))\n",
    "    enc_layers.append(('maxpool2d', {'kernel_size': 2}))\n",
    "\n",
    "enc_layers.append(('avgpool2d', {'kernel_size': (3,3)}))\n",
    "\n",
    "enc = EncoderCNN(enc_layers, hparams).to(device)\n",
    "dec = DecoderRNN(dataset.tokens, dataset.token_to_idx, 512, 512).to(device)\n",
    "\n",
    "model = EncoderDecoder(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED MODEL to cuda\n",
      "A\n",
      "torch.Size([31, 629])\n",
      "tensor(134, device='cuda:0')\n",
      "torch.Size([31, 134])\n",
      "torch.Size([31, 512])\n",
      "tensor([[546,   0,  51,  ..., 548, 548, 548],\n",
      "        [546,   0,  16,  ..., 548, 548, 548],\n",
      "        [546,   0, 147,  ..., 548, 548, 548],\n",
      "        ...,\n",
      "        [546,   0,  42,  ..., 548, 548, 548],\n",
      "        [546,   0,  21,  ..., 548, 548, 548],\n",
      "        [546,   0,   3,  ..., 548, 548, 548]], device='cuda:0')\n",
      "torch.Size([31, 134])\n",
      "torch.Size([31, 134, 512])\n",
      "torch.Size([31, 134, 512])\n",
      "torch.Size([31, 134, 549])\n",
      "H\n",
      "tensor([[False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True],\n",
      "        [False, False, False,  ...,  True,  True,  True]], device='cuda:0')\n",
      "torch.Size([2254, 549])\n",
      "torch.Size([2254, 549])\n",
      "h\n",
      "torch.Size([31, 134, 549])\n",
      "134\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = dataset.token_to_idx[PAD]\n",
    "\n",
    "def remove_trailing_pads(labels):\n",
    "   # Clip trailing PAD on labels\n",
    "   non_pad_cols = (labels != PAD_IDX).sum(dim=0)\n",
    "   non_pad_cols = non_pad_cols[non_pad_cols > 0]\n",
    "   \n",
    "   return labels[:, :len(non_pad_cols)]\n",
    "\n",
    "loader = data.DataLoader(dataset, batch_size = 31, shuffle = True)\n",
    "\n",
    "model_path = \"./models/model.pt\"\n",
    "current_params_path = \"./models/current_params.txt\"\n",
    "\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "print(f\"LOADED MODEL to {device}\")\n",
    "\n",
    "images, labels = next(iter(loader))\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "labels = remove_trailing_pads(labels)\n",
    "context_vec = model.encoder(images).squeeze()\n",
    "if len(context_vec.shape) == 1:\n",
    "    context_vec = context_vec.unsqueeze(0)\n",
    "print(context_vec.shape)\n",
    "print(labels)\n",
    "print(labels.shape)\n",
    "print(context_vec.unsqueeze(1).repeat(1, labels.shape[1], 1).shape)\n",
    "print(model.decoder.embedding(labels).shape)\n",
    "target = nn.functional.one_hot(labels, num_classes=len(dataset.tokens)).float().to(device)\n",
    "inputs = torch.cat([context_vec.unsqueeze(1).repeat(1, labels.shape[1], 1), model.decoder.embedding(labels)], dim=2)\n",
    "\n",
    "output, _ = model.decoder(inputs, None)\n",
    "print(output.shape)\n",
    "print(\"H\")\n",
    "mask = labels == PAD_IDX\n",
    "print(mask)\n",
    "print(output[labels == PAD_IDX].shape)\n",
    "print(target[labels == PAD_IDX].shape)\n",
    "output[labels == PAD_IDX] = 0\n",
    "target[labels == PAD_IDX] = 0\n",
    "print(\"h\")\n",
    "print(output.shape)\n",
    "output = output.argmax(dim=2)\n",
    "output_tokens = [dataset.tokens[token_idx] for token_idx in output[0].tolist()]\n",
    "print(len(output_tokens))\n",
    "print(len([dataset.tokens[token_idx] for token_idx in labels[0].tolist()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([31, 134, 549])\n",
      "torch.Size([31, 134])\n"
     ]
    }
   ],
   "source": [
    "print(target.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1172\n",
      "LOADED MODEL to cuda\n",
      "Running Batch 0, Epoch 0, Total Tokens: 125\n",
      "Loss: 14.389864921569824\n",
      "Running Batch 1, Epoch 0, Total Tokens: 152\n",
      "Loss: 12.044358253479004\n",
      "Running Batch 2, Epoch 0, Total Tokens: 135\n",
      "Loss: 12.803650856018066\n",
      "Running Batch 3, Epoch 0, Total Tokens: 121\n",
      "Loss: 11.371414184570312\n",
      "Running Batch 4, Epoch 0, Total Tokens: 162\n",
      "Loss: 11.098159790039062\n",
      "Running Batch 5, Epoch 0, Total Tokens: 134\n",
      "Loss: 10.201273918151855\n",
      "Running Batch 6, Epoch 0, Total Tokens: 139\n",
      "Loss: 9.952593803405762\n",
      "Running Batch 7, Epoch 0, Total Tokens: 135\n",
      "Loss: 8.683971405029297\n",
      "Running Batch 8, Epoch 0, Total Tokens: 140\n",
      "Loss: 8.14996337890625\n",
      "Running Batch 9, Epoch 0, Total Tokens: 126\n",
      "Loss: 6.841347694396973\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 9, Loss: 6.841347694396973\n",
      "Running Batch 10, Epoch 0, Total Tokens: 125\n",
      "Loss: 6.409285545349121\n",
      "Running Batch 11, Epoch 0, Total Tokens: 141\n",
      "Loss: 5.8944597244262695\n",
      "Running Batch 12, Epoch 0, Total Tokens: 123\n",
      "Loss: 5.115226745605469\n",
      "Running Batch 13, Epoch 0, Total Tokens: 120\n",
      "Loss: 4.564993858337402\n",
      "Running Batch 14, Epoch 0, Total Tokens: 122\n",
      "Loss: 4.653292179107666\n",
      "Running Batch 15, Epoch 0, Total Tokens: 143\n",
      "Loss: 5.001267433166504\n",
      "Running Batch 16, Epoch 0, Total Tokens: 262\n",
      "Loss: 5.109196186065674\n",
      "Running Batch 17, Epoch 0, Total Tokens: 164\n",
      "Loss: 4.978829860687256\n",
      "Running Batch 18, Epoch 0, Total Tokens: 203\n",
      "Loss: 4.667396068572998\n",
      "Running Batch 19, Epoch 0, Total Tokens: 137\n",
      "Loss: 4.598445415496826\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 19, Loss: 4.598445415496826\n",
      "Running Batch 20, Epoch 0, Total Tokens: 158\n",
      "Loss: 4.876801490783691\n",
      "Running Batch 21, Epoch 0, Total Tokens: 140\n",
      "Loss: 4.827706813812256\n",
      "Running Batch 22, Epoch 0, Total Tokens: 156\n",
      "Loss: 4.899634838104248\n",
      "Running Batch 23, Epoch 0, Total Tokens: 114\n",
      "Loss: 4.831046104431152\n",
      "Running Batch 24, Epoch 0, Total Tokens: 144\n",
      "Loss: 4.573366641998291\n",
      "Running Batch 25, Epoch 0, Total Tokens: 203\n",
      "Loss: 4.4998979568481445\n",
      "Running Batch 26, Epoch 0, Total Tokens: 156\n",
      "Loss: 4.276893615722656\n",
      "Running Batch 27, Epoch 0, Total Tokens: 129\n",
      "Loss: 4.410245895385742\n",
      "Running Batch 28, Epoch 0, Total Tokens: 143\n",
      "Loss: 4.337851047515869\n",
      "Running Batch 29, Epoch 0, Total Tokens: 130\n",
      "Loss: 4.241940975189209\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 29, Loss: 4.241940975189209\n",
      "Running Batch 30, Epoch 0, Total Tokens: 134\n",
      "Loss: 4.145849227905273\n",
      "Running Batch 31, Epoch 0, Total Tokens: 143\n",
      "Loss: 4.0079755783081055\n",
      "Running Batch 32, Epoch 0, Total Tokens: 129\n",
      "Loss: 4.039015293121338\n",
      "Running Batch 33, Epoch 0, Total Tokens: 152\n",
      "Loss: 4.0657267570495605\n",
      "Running Batch 34, Epoch 0, Total Tokens: 127\n",
      "Loss: 4.133403778076172\n",
      "Running Batch 35, Epoch 0, Total Tokens: 154\n",
      "Loss: 4.179566383361816\n",
      "Running Batch 36, Epoch 0, Total Tokens: 143\n",
      "Loss: 4.094481468200684\n",
      "Running Batch 37, Epoch 0, Total Tokens: 131\n",
      "Loss: 4.041376113891602\n",
      "Running Batch 38, Epoch 0, Total Tokens: 137\n",
      "Loss: 3.990257501602173\n",
      "Running Batch 39, Epoch 0, Total Tokens: 132\n",
      "Loss: 3.9932003021240234\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 39, Loss: 3.9932003021240234\n",
      "Running Batch 40, Epoch 0, Total Tokens: 144\n",
      "Loss: 3.9592089653015137\n",
      "Running Batch 41, Epoch 0, Total Tokens: 124\n",
      "Loss: 3.980128526687622\n",
      "Running Batch 42, Epoch 0, Total Tokens: 167\n",
      "Loss: 3.952814817428589\n",
      "Running Batch 43, Epoch 0, Total Tokens: 132\n",
      "Loss: 4.0078206062316895\n",
      "Running Batch 44, Epoch 0, Total Tokens: 120\n",
      "Loss: 3.9406728744506836\n",
      "Running Batch 45, Epoch 0, Total Tokens: 129\n",
      "Loss: 3.956306219100952\n",
      "Running Batch 46, Epoch 0, Total Tokens: 145\n",
      "Loss: 3.939567804336548\n",
      "Running Batch 47, Epoch 0, Total Tokens: 149\n",
      "Loss: 3.8404409885406494\n",
      "Running Batch 48, Epoch 0, Total Tokens: 299\n",
      "Loss: 3.9077582359313965\n",
      "Running Batch 49, Epoch 0, Total Tokens: 151\n",
      "Loss: 3.916682481765747\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 49, Loss: 3.916682481765747\n",
      "Running Batch 50, Epoch 0, Total Tokens: 144\n",
      "Loss: 3.8776280879974365\n",
      "Running Batch 51, Epoch 0, Total Tokens: 159\n",
      "Loss: 3.879218578338623\n",
      "Running Batch 52, Epoch 0, Total Tokens: 221\n",
      "Loss: 3.88930082321167\n",
      "Running Batch 53, Epoch 0, Total Tokens: 151\n",
      "Loss: 3.86953067779541\n",
      "Running Batch 54, Epoch 0, Total Tokens: 144\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\0Sem5\\774\\A4\\COL774-Ass_4-img2latex\\Q1_NonComp\\img2latex.ipynb Cell 8\u001b[0m line \u001b[0;36m5\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W3sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W3sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m), target\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W3sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m loss \u001b[39m=\u001b[39m loss[labels \u001b[39m!=\u001b[39;49m PAD_IDX]\u001b[39m.\u001b[39;49mmean()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W3sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W3sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(f\"Longest formula in training: {max([len(formula) for formula in dataset.data_frame['IndexList']])}\")\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
    "PAD_IDX = dataset.token_to_idx[PAD]\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "def remove_trailing_pads(labels):\n",
    "   # Clip trailing PAD on labels\n",
    "   non_pad_cols = (labels != PAD_IDX).sum(dim=0)\n",
    "   non_pad_cols = non_pad_cols[non_pad_cols > 0]\n",
    "\n",
    "   return labels[:, :len(non_pad_cols)]\n",
    "\n",
    "loader = data.DataLoader(dataset, batch_size = enc.hp[\"batch_size\"], shuffle = True)\n",
    "print(len(loader))\n",
    "model_path = \"./models/model.pt\"\n",
    "model_backup_path = \"./models/model_backup.pt\"\n",
    "current_params_path = \"./models/current_params.txt\" \n",
    "\n",
    "state_dict = torch.load(model_path)\n",
    "torch.save((state_dict), model_backup_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.train()\n",
    "print(f\"LOADED MODEL to {device}\")\n",
    "\n",
    "prev_loss = 1000000\n",
    "for epoch in range(1000):\n",
    "    curr_loss = 0\n",
    "    for bidx, batch in enumerate(loader):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        labels = remove_trailing_pads(labels)\n",
    "        context_vec = model.encoder(images).squeeze()\n",
    "\n",
    "        inputs = torch.cat([context_vec.unsqueeze(1).repeat(1, labels.shape[1], 1), model.decoder.embedding(labels)], dim=2)\n",
    "        print(f\"Running Batch {bidx}, Epoch {epoch}, Total Tokens: {labels.shape[1]}\")\n",
    "        output, _ = model.decoder(inputs, None)\n",
    "\n",
    "        # output[labels == PAD_IDX] = 0\n",
    "        # output = F.normalize(output, dim=2, p=1)\n",
    "        \n",
    "\n",
    "        target = nn.functional.one_hot(labels, num_classes=len(dataset.tokens)).float().to(device)\n",
    "        # target[labels == PAD_IDX] = 0\n",
    "        \n",
    "        # print(f\"Output shape: {output.shape}, Labels shape: {labels.shape}, Target shape: {target.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.transpose(1, 2), target.transpose(1, 2))\n",
    "        loss = loss[labels != PAD_IDX].mean()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        curr_loss += loss.item()\n",
    "        if bidx % 10 == 9:\n",
    "            print(f\"SAVING MODEL to {model_path}\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(\"SAVED MODEL\")\n",
    "            print(f\"Epoch: {epoch}, Batch: {bidx}, Loss: {loss.item()}\")\n",
    "            try:\n",
    "                with open(current_params_path, 'w') as f:\n",
    "                    f.write(f\"Epoch: {epoch}, Batch: {bidx}, Loss: {loss.item()}\")\n",
    "            except:\n",
    "                print(\"\\n Could not write to file \\n\")\n",
    "    print(f\"AVG LOSS: {(curr_loss)/len(loader)}, Epoch: {epoch+1}\")\n",
    "    prev_loss = curr_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
