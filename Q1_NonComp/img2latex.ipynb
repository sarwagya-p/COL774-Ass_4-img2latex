{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, layers, hparams):\n",
    "        '''\n",
    "        Args:\n",
    "            layers: Description of all layers in the Encoder: [(layer_type, {layer_params})]\n",
    "                - layer types - ['conv1d', 'conv2d', 'maxpool1d', 'maxpool2d', 'avgpool2d', 'avgpool2d', 'linear', 'dropout']\n",
    "                - layer_params - dict of parameters for the layer\n",
    "\n",
    "            hparams: Hyperparameters for the model\n",
    "        '''\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.hp = hparams\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for layer_type, layer_params in layers:\n",
    "            if layer_type == 'conv1d':\n",
    "                self.layers.append(nn.Conv1d(**layer_params))\n",
    "            elif layer_type == 'conv2d':\n",
    "                self.layers.append(nn.Conv2d(**layer_params))\n",
    "            elif layer_type == 'maxpool1d':\n",
    "                self.layers.append(nn.MaxPool1d(**layer_params))\n",
    "            elif layer_type == 'maxpool2d':\n",
    "                self.layers.append(nn.MaxPool2d(**layer_params))\n",
    "            elif layer_type == 'avgpool1d':\n",
    "                self.layers.append(nn.AvgPool1d(**layer_params))\n",
    "            elif layer_type == 'avgpool2d':\n",
    "                self.layers.append(nn.AvgPool2d(**layer_params))\n",
    "            elif layer_type == 'linear':\n",
    "                self.layers.append(nn.Linear(**layer_params))\n",
    "            elif layer_type == 'dropout':\n",
    "                self.layers.append(nn.Dropout(**layer_params))\n",
    "            else:\n",
    "                raise ValueError(f'Invalid layer type: {layer_type}')\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer(input)\n",
    "        return input\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab, vocab_dict, input_size, embedding_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            vocabulary_size: Size of the vocabulary\n",
    "            embedding_size: Size of the embedding vector\n",
    "        '''\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.vocab_dict = vocab_dict\n",
    "\n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_size)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.lstm = nn.LSTM(input_size+embedding_size, embedding_size, batch_first=True)\n",
    "        self.output = nn.Linear(embedding_size, len(vocab))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        '''\n",
    "        Args:\n",
    "            input: Input to the decoder\n",
    "            hidden: Hidden state of the previous time step\n",
    "        '''\n",
    "        # prev_embed = self.embedding(prev_tokens)\n",
    "        # concated_inp = torch.cat((input, prev_embed), dim=1)\n",
    "        if hidden is None:\n",
    "            output, hidden = self.lstm(input)\n",
    "        else:\n",
    "            output, hidden = self.lstm(input, hidden)\n",
    "        output = self.output(output)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def predict(self, input):\n",
    "        context_vec = self.encoder(input)\n",
    "        prev_token = torch.ones((input.shape[0]), dtype=int).cuda()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "PAD = \"<pad>\"\n",
    "SOS = \"<sos>\"\n",
    "EOS = \"<eos>\"\n",
    "\n",
    "def load_img(path, size = (224, 224)):\n",
    "    img = (Image.open(path))\n",
    "    transform = transforms.Compose([transforms.Resize(size, antialias=True), transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "    im = transform(img).detach()\n",
    "    im = 1 - im\n",
    "    return im\n",
    "\n",
    "class Img2LatexDataset(data.Dataset):\n",
    "    def __init__(self, img_dir, formula_path, img_size = (224, 224)):\n",
    "        self.data_frame = pd.read_csv(formula_path)\n",
    "        self.img_dir = img_dir\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.token_to_idx = {}\n",
    "        self.tokens = []\n",
    "\n",
    "        for row in self.data_frame[\"formula\"]:\n",
    "            row = row.split()\n",
    "\n",
    "            for token in row:\n",
    "                if token not in self.token_to_idx:\n",
    "                    self.token_to_idx[token] = len(self.token_to_idx)\n",
    "                    self.tokens.append(token)\n",
    "        \n",
    "        for special_token in [SOS, EOS, PAD]:\n",
    "            self.token_to_idx[special_token] = len(self.token_to_idx)\n",
    "            self.tokens.append(special_token)\n",
    "\n",
    "        max_len = max([len(row.split()) for row in self.data_frame[\"formula\"]])+2\n",
    "        def indexer(row):\n",
    "            index_list = [self.token_to_idx[SOS]]\n",
    "            index_list.extend([self.token_to_idx[token] for token in row.split()])\n",
    "            index_list.append(self.token_to_idx[EOS])\n",
    "            index_list.extend([self.token_to_idx[PAD]] * (max_len - len(index_list)))\n",
    "\n",
    "            return index_list\n",
    "        \n",
    "        self.data_frame[\"IndexList\"] = self.data_frame[\"formula\"].apply(indexer)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = load_img(self.img_dir + self.data_frame[\"image\"][index], self.img_size)\n",
    "        return img, torch.tensor(self.data_frame[\"IndexList\"][index], requires_grad=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.token_to_idx, self.tokens\n",
    "\n",
    "img_dir = \"../data/SyntheticData/images/\"\n",
    "formula_dir = \"../data/SyntheticData/train.csv\"\n",
    "\n",
    "dataset = Img2LatexDataset(img_dir, formula_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"lr\" : 0.001,\n",
    "    \"batch_size\" : 64,\n",
    "    \"epochs\" : 10\n",
    "}\n",
    "\n",
    "channel_seq = [3, 32, 64, 128, 256, 512]\n",
    "num_conv_pool = 5\n",
    "\n",
    "enc_layers = []\n",
    "\n",
    "for i in range(num_conv_pool):\n",
    "    enc_layers.append(('conv2d', {'in_channels': channel_seq[i], 'out_channels': channel_seq[i+1], 'kernel_size': 5}))\n",
    "    enc_layers.append(('maxpool2d', {'kernel_size': 2}))\n",
    "\n",
    "enc_layers.append(('avgpool2d', {'kernel_size': (3,3)}))\n",
    "\n",
    "enc = EncoderCNN(enc_layers, hparams).to(device)\n",
    "dec = DecoderRNN(dataset.tokens, dataset.token_to_idx, 512, 512).to(device)\n",
    "\n",
    "model = EncoderDecoder(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = dataset.token_to_idx[PAD]\n",
    "\n",
    "def remove_trailing_pads(labels):\n",
    "   # Clip trailing PAD on labels\n",
    "   non_pad_cols = (labels != PAD_IDX).sum(dim=0)\n",
    "   non_pad_cols = non_pad_cols[non_pad_cols > 0]\n",
    "   \n",
    "   return labels[:, :len(non_pad_cols)]\n",
    "\n",
    "loader = data.DataLoader(dataset, batch_size = 31, shuffle = True)\n",
    "\n",
    "model_path = \"./models/model.pt\"\n",
    "current_params_path = \"./models/current_params.txt\"\n",
    "\n",
    "state_dict = torch.load(model_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "print(f\"LOADED MODEL to {device}\")\n",
    "\n",
    "images, labels = next(iter(loader))\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "labels = remove_trailing_pads(labels)\n",
    "context_vec = model.encoder(images).squeeze()\n",
    "if len(context_vec.shape) == 1:\n",
    "    context_vec = context_vec.unsqueeze(0)\n",
    "print(context_vec.shape)\n",
    "print(labels)\n",
    "print(labels.shape)\n",
    "print(context_vec.unsqueeze(1).repeat(1, labels.shape[1], 1).shape)\n",
    "print(model.decoder.embedding(labels).shape)\n",
    "target = nn.functional.one_hot(labels, num_classes=len(dataset.tokens)).float().to(device)\n",
    "inputs = torch.cat([context_vec.unsqueeze(1).repeat(1, labels.shape[1], 1), model.decoder.embedding(labels)], dim=2)\n",
    "\n",
    "output, _ = model.decoder(inputs, None)\n",
    "print(output.shape)\n",
    "print(\"H\")\n",
    "mask = labels == PAD_IDX\n",
    "print(mask)\n",
    "print(output[labels == PAD_IDX].shape)\n",
    "print(target[labels == PAD_IDX].shape)\n",
    "# output[labels == PAD_IDX] = 0\n",
    "# target[labels == PAD_IDX] = 0\n",
    "print(\"h\")\n",
    "print(output.shape)\n",
    "output = output.argmax(dim=2)\n",
    "output_tokens = [dataset.tokens[token_idx] for token_idx in output[0].tolist()]\n",
    "print(output_tokens)\n",
    "print([dataset.tokens[token_idx] for token_idx in labels[0, 1:].tolist()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 27\n",
    "output_tokens = [dataset.tokens[token_idx] for token_idx in output[t].tolist()]\n",
    "print(output_tokens)\n",
    "print([dataset.tokens[token_idx] for token_idx in labels[t, 1:].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_vec.std(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Longest formula in training: {max([len(formula) for formula in dataset.data_frame['IndexList']])}\")\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "PAD_IDX = dataset.token_to_idx[PAD]\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "def remove_trailing_pads(labels):\n",
    "   # Clip trailing PAD on labels\n",
    "   non_pad_cols = (labels != PAD_IDX).sum(dim=0)\n",
    "   non_pad_cols = non_pad_cols[non_pad_cols > 0]\n",
    "\n",
    "   return labels[:, :len(non_pad_cols)]\n",
    "\n",
    "loader = data.DataLoader(dataset, batch_size = enc.hp[\"batch_size\"], shuffle = True)\n",
    "print(len(loader))\n",
    "model_path = \"./models/model.pt\"\n",
    "model_backup_path = \"./models/model_backup.pt\"\n",
    "current_params_path = \"./models/current_params.txt\" \n",
    "\n",
    "state_dict = torch.load(model_path)\n",
    "torch.save((state_dict), model_backup_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.train()\n",
    "print(f\"LOADED MODEL to {device}\")\n",
    "\n",
    "prev_loss = 100\n",
    "for epoch in range(100):\n",
    "    curr_loss = 0\n",
    "    for bidx, batch in enumerate(loader):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        labels = remove_trailing_pads(labels)\n",
    "        context_vec = model.encoder(images).squeeze()\n",
    "\n",
    "        inputs = torch.cat([context_vec.unsqueeze(1).repeat(1, labels.shape[1], 1), model.decoder.embedding(labels)], dim=2)\n",
    "        print(f\"Running Batch {bidx}, Epoch {epoch}, Total Tokens: {labels.shape[1]}\")\n",
    "        output, _ = model.decoder(inputs, None)\n",
    "\n",
    "        # output[labels == PAD_IDX] = 0\n",
    "        # output = F.normalize(output, dim=2, p=1)\n",
    "        output = output[:, :-1, :]\n",
    "\n",
    "        target = nn.functional.one_hot(labels[:,1:], num_classes=len(dataset.tokens)).float().to(device)\n",
    "        # target[labels == PAD_IDX] = 0\n",
    "        \n",
    "        # print(f\"Output shape: {output.shape}, Labels shape: {labels.shape}, Target shape: {target.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.transpose(1, 2), target.transpose(1, 2))\n",
    "        loss = loss[labels[:,1:] != PAD_IDX].mean()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         print(f\"Layer: {name}, Mean: {param.grad.mean()}, Std: {param.grad.std()}\")\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        curr_loss += loss.item()\n",
    "        if bidx % 10 == 9:\n",
    "            print(f\"SAVING MODEL to {model_path}\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(\"SAVED MODEL\")\n",
    "            print(f\"Epoch: {epoch}, Batch: {bidx}, Loss: {loss.item()}\")\n",
    "            try:\n",
    "                with open(current_params_path, 'w') as f:\n",
    "                    f.write(f\"Epoch: {epoch}, Batch: {bidx}, Loss: {loss.item()}\")\n",
    "            except:\n",
    "                print(\"\\n Could not write to file \\n\")\n",
    "    print(f\"AVG LOSS: {(curr_loss)/len(loader)}, Epoch: {epoch+1}\")\n",
    "    prev_loss = curr_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training without teacher enforcing\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "PAD_IDX = dataset.token_to_idx[PAD]\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "def remove_trailing_pads(labels):\n",
    "   # Clip trailing PAD on labels\n",
    "   non_pad_cols = (labels != PAD_IDX).sum(dim=0)\n",
    "   non_pad_cols = non_pad_cols[non_pad_cols > 0]\n",
    "\n",
    "   return labels[:, :len(non_pad_cols)]\n",
    "\n",
    "loader = data.DataLoader(dataset, batch_size = enc.hp[\"batch_size\"], shuffle = True)\n",
    "print(len(loader))\n",
    "model_path = \"./models/model.pt\"\n",
    "model_backup_path = \"./models/model_backup.pt\"\n",
    "current_params_path = \"./models/current_params.txt\" \n",
    "\n",
    "state_dict = torch.load(model_path, map_location=torch.device(device))\n",
    "torch.save((state_dict), model_backup_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.train()\n",
    "print(f\"LOADED MODEL to {device}\")\n",
    "\n",
    "prev_loss = 100\n",
    "for epoch in range(100):\n",
    "    curr_loss = 0\n",
    "    for bidx, batch in enumerate(loader):\n",
    "        print(f\"Running Batch {bidx}, Epoch {epoch}\")\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        labels = remove_trailing_pads(labels)\n",
    "        context_vec = model.encoder(images).squeeze()\n",
    "\n",
    "        output = torch.zeros((labels.shape[0], labels.shape[1]-1, len(dataset.tokens))).to(device)\n",
    "\n",
    "        prev_token = torch.ones(labels.shape[0], dtype=int).to(device) * dataset.token_to_idx[SOS]\n",
    "        prev_token_embed = model.decoder.embedding(prev_token).to(device)\n",
    "\n",
    "        input = torch.cat([context_vec, prev_token_embed], dim=1).to(device)\n",
    "        hidden = None\n",
    "\n",
    "        for i in range(labels.shape[1]-1):\n",
    "            output[:, i, :], hidden = model.decoder(input, hidden)\n",
    "            prev_token = output[:, i, :].argmax(dim=1)\n",
    "            prev_token_embed = model.decoder.embedding(prev_token)\n",
    "            input = torch.cat([context_vec, prev_token_embed], dim=1).to(device)\n",
    "\n",
    "        target = nn.functional.one_hot(labels[:,1:], num_classes=len(dataset.tokens)).float().to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.transpose(1, 2), target.transpose(1, 2))\n",
    "        loss = loss[labels[:,1:] != PAD_IDX].mean()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        curr_loss += loss.item()\n",
    "        if bidx % 10 == 9:\n",
    "            print(f\"SAVING MODEL to {model_path}\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(\"SAVED MODEL\")\n",
    "            print(f\"Epoch: {epoch}, Batch: {bidx}, Loss: {loss.item()}\")\n",
    "            try:\n",
    "                with open(current_params_path, 'w') as f:\n",
    "                    f.write(f\"Epoch: {epoch}, Batch: {bidx}, Loss: {loss.item()}\")\n",
    "            except:\n",
    "                print(\"\\n Could not write to file \\n\")\n",
    "    print(f\"AVG LOSS: {(curr_loss)/len(loader)}, Epoch: {epoch+1}\")\n",
    "    prev_loss = curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
