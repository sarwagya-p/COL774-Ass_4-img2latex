{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, layers, hparams):\n",
    "        '''\n",
    "        Args:\n",
    "            layers: Description of all layers in the Encoder: [(layer_type, {layer_params})]\n",
    "                - layer types - ['conv1d', 'conv2d', 'maxpool1d', 'maxpool2d', 'avgpool2d', 'avgpool2d', 'linear', 'dropout']\n",
    "                - layer_params - dict of parameters for the layer\n",
    "\n",
    "            hparams: Hyperparameters for the model\n",
    "        '''\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.hp = hparams\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        for layer_type, layer_params in layers:\n",
    "            if layer_type == 'conv1d':\n",
    "                self.layers.append(nn.Conv1d(**layer_params))\n",
    "            elif layer_type == 'conv2d':\n",
    "                self.layers.append(nn.Conv2d(**layer_params))\n",
    "            elif layer_type == 'maxpool1d':\n",
    "                self.layers.append(nn.MaxPool1d(**layer_params))\n",
    "            elif layer_type == 'maxpool2d':\n",
    "                self.layers.append(nn.MaxPool2d(**layer_params))\n",
    "            elif layer_type == 'avgpool1d':\n",
    "                self.layers.append(nn.AvgPool1d(**layer_params))\n",
    "            elif layer_type == 'avgpool2d':\n",
    "                self.layers.append(nn.AvgPool2d(**layer_params))\n",
    "            elif layer_type == 'linear':\n",
    "                self.layers.append(nn.Linear(**layer_params))\n",
    "            elif layer_type == 'dropout':\n",
    "                self.layers.append(nn.Dropout(**layer_params))\n",
    "            else:\n",
    "                raise ValueError(f'Invalid layer type: {layer_type}')\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer(input)\n",
    "        return input\n",
    "    \n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab, vocab_dict, input_size, embedding_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        '''\n",
    "        Args:\n",
    "            vocabulary_size: Size of the vocabulary\n",
    "            embedding_size: Size of the embedding vector\n",
    "        '''\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.vocab_dict = vocab_dict\n",
    "\n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_size)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.lstm = nn.LSTM(input_size+embedding_size, embedding_size, batch_first=True)\n",
    "        self.output = nn.Linear(embedding_size, len(vocab))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        '''\n",
    "        Args:\n",
    "            input: Input to the decoder\n",
    "            hidden: Hidden state of the previous time step\n",
    "        '''\n",
    "        # prev_embed = self.embedding(prev_tokens)\n",
    "        # concated_inp = torch.cat((input, prev_embed), dim=1)\n",
    "        if hidden is None:\n",
    "            output, hidden = self.lstm(input)\n",
    "        else:\n",
    "            output, hidden = self.lstm(input, hidden)\n",
    "        output = self.output(output)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def predict(self, input):\n",
    "        context_vec = self.encoder(input)\n",
    "        prev_token = torch.ones((input.shape[0]), dtype=int).cuda()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "PAD = \"<pad>\"\n",
    "SOS = \"<sos>\"\n",
    "EOS = \"<eos>\"\n",
    "\n",
    "def load_img(path, size = (224, 224)):\n",
    "    img = (Image.open(path))\n",
    "    transform = transforms.Compose([transforms.Resize(size, antialias=True), transforms.ToTensor(), transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n",
    "    im = transform(img).detach()\n",
    "    im = 1 - im\n",
    "    return im\n",
    "\n",
    "class Img2LatexDataset(data.Dataset):\n",
    "    def __init__(self, img_dir, formula_path, img_size = (224, 224)):\n",
    "        self.data_frame = pd.read_csv(formula_path)\n",
    "        self.img_dir = img_dir\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.token_to_idx = {}\n",
    "        self.tokens = []\n",
    "\n",
    "        for row in self.data_frame[\"formula\"]:\n",
    "            row = row.split()\n",
    "\n",
    "            for token in row:\n",
    "                if token not in self.token_to_idx:\n",
    "                    self.token_to_idx[token] = len(self.token_to_idx)\n",
    "                    self.tokens.append(token)\n",
    "        \n",
    "        for special_token in [SOS, EOS, PAD]:\n",
    "            self.token_to_idx[special_token] = len(self.token_to_idx)\n",
    "            self.tokens.append(special_token)\n",
    "\n",
    "        max_len = max([len(row.split()) for row in self.data_frame[\"formula\"]])+2\n",
    "        def indexer(row):\n",
    "            index_list = [self.token_to_idx[SOS]]\n",
    "            index_list.extend([self.token_to_idx[token] for token in row.split()])\n",
    "            index_list.append(self.token_to_idx[EOS])\n",
    "            index_list.extend([self.token_to_idx[PAD]] * (max_len - len(index_list)))\n",
    "\n",
    "            return index_list\n",
    "        \n",
    "        self.data_frame[\"IndexList\"] = self.data_frame[\"formula\"].apply(indexer)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = load_img(self.img_dir + self.data_frame[\"image\"][index], self.img_size)\n",
    "        return img, torch.tensor(self.data_frame[\"IndexList\"][index], requires_grad=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.token_to_idx, self.tokens\n",
    "\n",
    "img_dir = \"../data/SyntheticData/images/\"\n",
    "formula_dir = \"../data/SyntheticData/train.csv\"\n",
    "\n",
    "dataset = Img2LatexDataset(img_dir, formula_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"lr\" : 0.0001,\n",
    "    \"batch_size\" : 96,\n",
    "    \"epochs\" : 10\n",
    "}\n",
    "\n",
    "channel_seq = [3, 32, 64, 128, 256, 512]\n",
    "num_conv_pool = 5\n",
    "\n",
    "enc_layers = []\n",
    "\n",
    "for i in range(num_conv_pool):\n",
    "    enc_layers.append(('conv2d', {'in_channels': channel_seq[i], 'out_channels': channel_seq[i+1], 'kernel_size': 5}))\n",
    "    enc_layers.append(('maxpool2d', {'kernel_size': 2}))\n",
    "\n",
    "enc_layers.append(('avgpool2d', {'kernel_size': (3,3)}))\n",
    "\n",
    "enc = EncoderCNN(enc_layers, hparams).to(device)\n",
    "dec = DecoderRNN(dataset.tokens, dataset.token_to_idx, 512, 512).to(device)\n",
    "\n",
    "model = EncoderDecoder(enc, dec).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(type(param.data), param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782\n",
      "LOADED MODEL to cuda\n",
      "Running Batch 0, Epoch 0, Total Tokens: 171\n",
      "Loss: 6.324601650238037\n",
      "Running Batch 1, Epoch 0, Total Tokens: 135\n",
      "Loss: 6.2754950523376465\n",
      "Running Batch 2, Epoch 0, Total Tokens: 150\n",
      "Loss: 6.211289882659912\n",
      "Running Batch 3, Epoch 0, Total Tokens: 186\n",
      "Loss: 6.125494480133057\n",
      "Running Batch 4, Epoch 0, Total Tokens: 203\n",
      "Loss: 5.97739839553833\n",
      "Running Batch 5, Epoch 0, Total Tokens: 224\n",
      "Loss: 5.7900309562683105\n",
      "Running Batch 6, Epoch 0, Total Tokens: 143\n",
      "Loss: 5.58447790145874\n",
      "Running Batch 7, Epoch 0, Total Tokens: 143\n",
      "Loss: 5.351614952087402\n",
      "Running Batch 8, Epoch 0, Total Tokens: 176\n",
      "Loss: 5.184464931488037\n",
      "Running Batch 9, Epoch 0, Total Tokens: 146\n",
      "Loss: 4.966055870056152\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 9, Loss: 4.966055870056152\n",
      "Running Batch 10, Epoch 0, Total Tokens: 126\n",
      "Loss: 4.906158924102783\n",
      "Running Batch 11, Epoch 0, Total Tokens: 140\n",
      "Loss: 4.785327911376953\n",
      "Running Batch 12, Epoch 0, Total Tokens: 155\n",
      "Loss: 4.722977161407471\n",
      "Running Batch 13, Epoch 0, Total Tokens: 123\n",
      "Loss: 4.650395393371582\n",
      "Running Batch 14, Epoch 0, Total Tokens: 165\n",
      "Loss: 4.573109149932861\n",
      "Running Batch 15, Epoch 0, Total Tokens: 173\n",
      "Loss: 4.604482173919678\n",
      "Running Batch 16, Epoch 0, Total Tokens: 135\n",
      "Loss: 4.445338249206543\n",
      "Running Batch 17, Epoch 0, Total Tokens: 146\n",
      "Loss: 4.446345329284668\n",
      "Running Batch 18, Epoch 0, Total Tokens: 221\n",
      "Loss: 4.329981327056885\n",
      "Running Batch 19, Epoch 0, Total Tokens: 134\n",
      "Loss: 4.332796573638916\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 19, Loss: 4.332796573638916\n",
      "Running Batch 20, Epoch 0, Total Tokens: 154\n",
      "Loss: 4.313719272613525\n",
      "Running Batch 21, Epoch 0, Total Tokens: 258\n",
      "Loss: 4.25339937210083\n",
      "Running Batch 22, Epoch 0, Total Tokens: 220\n",
      "Loss: 4.222679138183594\n",
      "Running Batch 23, Epoch 0, Total Tokens: 152\n",
      "Loss: 4.168735980987549\n",
      "Running Batch 24, Epoch 0, Total Tokens: 126\n",
      "Loss: 4.218560218811035\n",
      "Running Batch 25, Epoch 0, Total Tokens: 153\n",
      "Loss: 4.192113876342773\n",
      "Running Batch 26, Epoch 0, Total Tokens: 129\n",
      "Loss: 4.102828025817871\n",
      "Running Batch 27, Epoch 0, Total Tokens: 152\n",
      "Loss: 4.11676025390625\n",
      "Running Batch 28, Epoch 0, Total Tokens: 494\n",
      "Loss: 4.095148086547852\n",
      "Running Batch 29, Epoch 0, Total Tokens: 158\n",
      "Loss: 4.149747371673584\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 29, Loss: 4.149747371673584\n",
      "Running Batch 30, Epoch 0, Total Tokens: 135\n",
      "Loss: 4.054579257965088\n",
      "Running Batch 31, Epoch 0, Total Tokens: 150\n",
      "Loss: 4.024611949920654\n",
      "Running Batch 32, Epoch 0, Total Tokens: 199\n",
      "Loss: 4.082050800323486\n",
      "Running Batch 33, Epoch 0, Total Tokens: 145\n",
      "Loss: 4.031590938568115\n",
      "Running Batch 34, Epoch 0, Total Tokens: 144\n",
      "Loss: 3.9945898056030273\n",
      "Running Batch 35, Epoch 0, Total Tokens: 144\n",
      "Loss: 3.987004280090332\n",
      "Running Batch 36, Epoch 0, Total Tokens: 129\n",
      "Loss: 3.9772770404815674\n",
      "Running Batch 37, Epoch 0, Total Tokens: 155\n",
      "Loss: 4.003037929534912\n",
      "Running Batch 38, Epoch 0, Total Tokens: 140\n",
      "Loss: 3.9172675609588623\n",
      "Running Batch 39, Epoch 0, Total Tokens: 162\n",
      "Loss: 3.9213640689849854\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 39, Loss: 3.9213640689849854\n",
      "Running Batch 40, Epoch 0, Total Tokens: 135\n",
      "Loss: 3.966907024383545\n",
      "Running Batch 41, Epoch 0, Total Tokens: 190\n",
      "Loss: 3.9122908115386963\n",
      "Running Batch 42, Epoch 0, Total Tokens: 173\n",
      "Loss: 3.9539074897766113\n",
      "Running Batch 43, Epoch 0, Total Tokens: 149\n",
      "Loss: 3.834451198577881\n",
      "Running Batch 44, Epoch 0, Total Tokens: 188\n",
      "Loss: 3.8667051792144775\n",
      "Running Batch 45, Epoch 0, Total Tokens: 285\n",
      "Loss: 3.885470390319824\n",
      "Running Batch 46, Epoch 0, Total Tokens: 169\n",
      "Loss: 3.8484699726104736\n",
      "Running Batch 47, Epoch 0, Total Tokens: 162\n",
      "Loss: 3.8759946823120117\n",
      "Running Batch 48, Epoch 0, Total Tokens: 154\n",
      "Loss: 3.8467464447021484\n",
      "Running Batch 49, Epoch 0, Total Tokens: 145\n",
      "Loss: 3.833902359008789\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 49, Loss: 3.833902359008789\n",
      "Running Batch 50, Epoch 0, Total Tokens: 143\n",
      "Loss: 3.858701705932617\n",
      "Running Batch 51, Epoch 0, Total Tokens: 135\n",
      "Loss: 3.8080615997314453\n",
      "Running Batch 52, Epoch 0, Total Tokens: 131\n",
      "Loss: 3.8192129135131836\n",
      "Running Batch 53, Epoch 0, Total Tokens: 179\n",
      "Loss: 3.7499337196350098\n",
      "Running Batch 54, Epoch 0, Total Tokens: 190\n",
      "Loss: 3.833662509918213\n",
      "Running Batch 55, Epoch 0, Total Tokens: 152\n",
      "Loss: 3.7283005714416504\n",
      "Running Batch 56, Epoch 0, Total Tokens: 173\n",
      "Loss: 3.796344518661499\n",
      "Running Batch 57, Epoch 0, Total Tokens: 152\n",
      "Loss: 3.773562431335449\n",
      "Running Batch 58, Epoch 0, Total Tokens: 152\n",
      "Loss: 3.7798774242401123\n",
      "Running Batch 59, Epoch 0, Total Tokens: 151\n",
      "Loss: 3.708883047103882\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 59, Loss: 3.708883047103882\n",
      "Running Batch 60, Epoch 0, Total Tokens: 146\n",
      "Loss: 3.71433687210083\n",
      "Running Batch 61, Epoch 0, Total Tokens: 452\n",
      "Loss: 3.7556796073913574\n",
      "Running Batch 62, Epoch 0, Total Tokens: 146\n",
      "Loss: 3.7761049270629883\n",
      "Running Batch 63, Epoch 0, Total Tokens: 126\n",
      "Loss: 3.738715410232544\n",
      "Running Batch 64, Epoch 0, Total Tokens: 146\n",
      "Loss: 3.6882922649383545\n",
      "Running Batch 65, Epoch 0, Total Tokens: 135\n",
      "Loss: 3.7092204093933105\n",
      "Running Batch 66, Epoch 0, Total Tokens: 143\n",
      "Loss: 3.6938605308532715\n",
      "Running Batch 67, Epoch 0, Total Tokens: 196\n",
      "Loss: 3.714845895767212\n",
      "Running Batch 68, Epoch 0, Total Tokens: 130\n",
      "Loss: 3.7173874378204346\n",
      "Running Batch 69, Epoch 0, Total Tokens: 244\n",
      "Loss: 3.7031378746032715\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 69, Loss: 3.7031378746032715\n",
      "Running Batch 70, Epoch 0, Total Tokens: 143\n",
      "Loss: 3.7031936645507812\n",
      "Running Batch 71, Epoch 0, Total Tokens: 135\n",
      "Loss: 3.688972234725952\n",
      "Running Batch 72, Epoch 0, Total Tokens: 351\n",
      "Loss: 3.701388120651245\n",
      "Running Batch 73, Epoch 0, Total Tokens: 134\n",
      "Loss: 3.677825450897217\n",
      "Running Batch 74, Epoch 0, Total Tokens: 154\n",
      "Loss: 3.553906202316284\n",
      "Running Batch 75, Epoch 0, Total Tokens: 157\n",
      "Loss: 3.6559360027313232\n",
      "Running Batch 76, Epoch 0, Total Tokens: 299\n",
      "Loss: 3.7246315479278564\n",
      "Running Batch 77, Epoch 0, Total Tokens: 130\n",
      "Loss: 3.62934947013855\n",
      "Running Batch 78, Epoch 0, Total Tokens: 154\n",
      "Loss: 3.6168901920318604\n",
      "Running Batch 79, Epoch 0, Total Tokens: 172\n",
      "Loss: 3.6264498233795166\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 79, Loss: 3.6264498233795166\n",
      "Running Batch 80, Epoch 0, Total Tokens: 151\n",
      "Loss: 3.544402837753296\n",
      "Running Batch 81, Epoch 0, Total Tokens: 149\n",
      "Loss: 3.6302006244659424\n",
      "Running Batch 82, Epoch 0, Total Tokens: 428\n",
      "Loss: 3.633577823638916\n",
      "Running Batch 83, Epoch 0, Total Tokens: 296\n",
      "Loss: 3.646749258041382\n",
      "Running Batch 84, Epoch 0, Total Tokens: 147\n",
      "Loss: 3.5820484161376953\n",
      "Running Batch 85, Epoch 0, Total Tokens: 140\n",
      "Loss: 3.6905856132507324\n",
      "Running Batch 86, Epoch 0, Total Tokens: 152\n",
      "Loss: 3.5920910835266113\n",
      "Running Batch 87, Epoch 0, Total Tokens: 148\n",
      "Loss: 3.5042757987976074\n",
      "Running Batch 88, Epoch 0, Total Tokens: 357\n",
      "Loss: 3.6123759746551514\n",
      "Running Batch 89, Epoch 0, Total Tokens: 146\n",
      "Loss: 3.4958744049072266\n",
      "SAVING MODEL to ./models/model.pt\n",
      "SAVED MODEL\n",
      "Epoch: 0, Batch: 89, Loss: 3.4958744049072266\n",
      "Running Batch 90, Epoch 0, Total Tokens: 121\n",
      "Loss: 3.567662000656128\n",
      "Running Batch 91, Epoch 0, Total Tokens: 139\n",
      "Loss: 3.518120288848877\n",
      "Running Batch 92, Epoch 0, Total Tokens: 188\n",
      "Loss: 3.5280890464782715\n",
      "Running Batch 93, Epoch 0, Total Tokens: 134\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\0Sem5\\774\\A4\\COL774-Ass_4-img2latex\\Q1_NonComp\\img2latex.ipynb Cell 7\u001b[0m line \u001b[0;36m8\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W6sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W6sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \u001b[39m# for name, param in model.named_parameters():\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W6sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39m#     if param.requires_grad:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W6sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m#         print(f\"Layer: {name}, Mean: {param.grad.mean()}, Std: {param.grad.std()}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W6sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W6sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39m# optimizer.zero_grad()\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W6sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39;49mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W6sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m curr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/0Sem5/774/A4/COL774-Ass_4-img2latex/Q1_NonComp/img2latex.ipynb#W6sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mif\u001b[39;00m bidx \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m9\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# print(f\"Longest formula in training: {max([len(formula) for formula in dataset.data_frame['IndexList']])}\")\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "PAD_IDX = dataset.token_to_idx[PAD]\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "def remove_trailing_pads(labels):\n",
    "   # Clip trailing PAD on labels\n",
    "   non_pad_cols = (labels != PAD_IDX).sum(dim=0)\n",
    "   non_pad_cols = non_pad_cols[non_pad_cols > 0]\n",
    "\n",
    "   return labels[:, :len(non_pad_cols)]\n",
    "\n",
    "loader = data.DataLoader(dataset, batch_size = enc.hp[\"batch_size\"], shuffle = True)\n",
    "print(len(loader))\n",
    "model_path = \"./models/model.pt\"\n",
    "model_backup_path = \"./models/model_backup.pt\"\n",
    "current_params_path = \"./models/current_params.txt\" \n",
    "\n",
    "state_dict = torch.load(model_path)\n",
    "torch.save((state_dict), model_backup_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.train()\n",
    "print(f\"LOADED MODEL to {device}\")\n",
    "\n",
    "fifty_fifty = False\n",
    "teacher_forcing = True\n",
    "\n",
    "prev_loss = 100\n",
    "for epoch in range(8):\n",
    "    curr_loss = 0\n",
    "    for bidx, batch in enumerate(loader):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        labels = remove_trailing_pads(labels)\n",
    "        context_vec = model.encoder(images).squeeze()\n",
    "        if (bidx%2 and fifty_fifty) or teacher_forcing:\n",
    "            inputs = torch.cat([context_vec.unsqueeze(1).repeat(1, labels.shape[1], 1), model.decoder.embedding(labels)], dim=2)\n",
    "            print(f\"Running Batch {bidx}, Epoch {epoch}, Total Tokens: {labels.shape[1]}\")\n",
    "            output, _ = model.decoder(inputs, None)\n",
    "\n",
    "            # output[labels == PAD_IDX] = 0\n",
    "            # output = F.normalize(output, dim=2, p=1)\n",
    "            output = output[:, :-1, :]\n",
    "\n",
    "        else:\n",
    "            output = torch.zeros((labels.shape[0], labels.shape[1]-1, len(dataset.tokens))).to(device)\n",
    "\n",
    "            prev_token = torch.ones(labels.shape[0], dtype=int).to(device) * dataset.token_to_idx[SOS]\n",
    "            prev_token_embed = model.decoder.embedding(prev_token).to(device)\n",
    "\n",
    "            input = torch.cat([context_vec, prev_token_embed], dim=1).to(device)\n",
    "            hidden = None\n",
    "\n",
    "            for i in range(labels.shape[1]-1):\n",
    "                output[:, i, :], hidden = model.decoder(input, hidden)\n",
    "                prev_token = output[:, i, :].argmax(dim=1)\n",
    "                prev_token_embed = model.decoder.embedding(prev_token)\n",
    "                input = torch.cat([context_vec, prev_token_embed], dim=1).to(device)\n",
    "            \n",
    "        target = nn.functional.one_hot(labels[:,1:], num_classes=len(dataset.tokens)).float().to(device)\n",
    "        # target[labels == PAD_IDX] = 0\n",
    "        mask = labels[:,1:] != PAD_IDX\n",
    "        \n",
    "        # print(f\"Output shape: {output.shape}, Labels shape: {labels.shape}, Target shape: {target.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.transpose(1, 2), target.transpose(1, 2))\n",
    "        loss = loss * mask\n",
    "        loss = loss.sum() / mask.sum()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         print(f\"Layer: {name}, Mean: {param.grad.mean()}, Std: {param.grad.std()}\")\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        curr_loss += loss.item()\n",
    "        if bidx % 10 == 9:\n",
    "            print(f\"SAVING MODEL to {model_path}\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(\"SAVED MODEL\")\n",
    "            print(f\"Epoch: {epoch}, Batch: {bidx}, Loss: {loss.item()}\")\n",
    "            try:\n",
    "                with open(current_params_path, 'w') as f:\n",
    "                    f.write(f\"Epoch: {epoch}, Batch: {bidx}, Loss: {loss.item()}\")\n",
    "            except:\n",
    "                print(\"\\n Could not write to file \\n\")\n",
    "    print(f\"AVG LOSS: {(curr_loss)/len(loader)}, Epoch: {epoch+1}\")\n",
    "    prev_loss = curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/model_50synth.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning\n",
    "handwritten_imgs = \"../data/HandwrittenData/images/\"\n",
    "handwritten_labels = \"../data/HandwrittenData/train_hw.csv\"\n",
    "\n",
    "handwritten_dataset = Img2LatexDataset(handwritten_imgs, handwritten_labels)\n",
    "handwritten_loader = data.DataLoader(handwritten_dataset, batch_size=64, shuffle=True)\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.encoder.parameters(), lr = 0.001)\n",
    "PAD_IDX = handwritten_dataset.token_to_idx[PAD]\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "def remove_trailing_pads(labels):\n",
    "   # Clip trailing PAD on labels\n",
    "   non_pad_cols = (labels != PAD_IDX).sum(dim=0)\n",
    "   non_pad_cols = non_pad_cols[non_pad_cols > 0]\n",
    "\n",
    "   return labels[:, :len(non_pad_cols)]\n",
    "\n",
    "print(len(handwritten_loader))\n",
    "model_path = \"./models/model_hw.pt\"\n",
    "model_backup_path = \"./models/model_hw_backup.pt\"\n",
    "current_params_path = \"./models/current_params_hw.txt\" \n",
    "\n",
    "state_dict = torch.load(\"./models/model_synth.pt\")\n",
    "torch.save((state_dict), model_backup_path)\n",
    "model.load_state_dict(state_dict)\n",
    "model.train()\n",
    "print(f\"LOADED MODEL to {device}\")\n",
    "\n",
    "fifty_fifty = True\n",
    "teacher_forcing = False\n",
    "\n",
    "prev_loss = 100\n",
    "for epoch in range(1):\n",
    "    curr_loss = 0\n",
    "    for bidx, batch in enumerate(handwritten_loader):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        labels = remove_trailing_pads(labels)\n",
    "        context_vec = model.encoder(images).squeeze()\n",
    "        if (bidx%2 and fifty_fifty) or teacher_forcing:\n",
    "            inputs = torch.cat([context_vec.unsqueeze(1).repeat(1, labels.shape[1], 1), model.decoder.embedding(labels)], dim=2)\n",
    "            print(f\"Running Batch {bidx}, Epoch {epoch}, Total Tokens: {labels.shape[1]}\")\n",
    "            output, _ = model.decoder(inputs, None)\n",
    "\n",
    "            # output[labels == PAD_IDX] = 0\n",
    "            # output = F.normalize(output, dim=2, p=1)\n",
    "            output = output[:, :-1, :]\n",
    "\n",
    "        else:\n",
    "            output = torch.zeros((labels.shape[0], labels.shape[1]-1, len(handwritten_dataset.tokens))).to(device)\n",
    "\n",
    "            prev_token = torch.ones(labels.shape[0], dtype=int).to(device) * handwritten_dataset.token_to_idx[SOS]\n",
    "            prev_token_embed = model.decoder.embedding(prev_token).to(device)\n",
    "\n",
    "            input = torch.cat([context_vec, prev_token_embed], dim=1).to(device)\n",
    "            hidden = None\n",
    "\n",
    "            for i in range(labels.shape[1]-1):\n",
    "                output[:, i, :], hidden = model.decoder(input, hidden)\n",
    "                prev_token = output[:, i, :].argmax(dim=1)\n",
    "                prev_token_embed = model.decoder.embedding(prev_token)\n",
    "                input = torch.cat([context_vec, prev_token_embed], dim=1).to(device)\n",
    "            \n",
    "        target = nn.functional.one_hot(labels[:,1:], num_classes=len(handwritten_dataset.tokens)).float().to(device)\n",
    "        # target[labels == PAD_IDX] = 0\n",
    "        mask = labels[:,1:] != PAD_IDX\n",
    "        # print(f\"Output shape: {output.shape}, Labels shape: {labels.shape}, Target shape: {target.shape}\")\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.transpose(1, 2), target.transpose(1, 2))\n",
    "        loss = loss * mask\n",
    "        loss = loss.sum() / mask.sum()\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.requires_grad:\n",
    "        #         print(f\"Layer: {name}, Mean: {param.grad.mean()}, Std: {param.grad.std()}\")\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "        curr_loss += loss.item()\n",
    "        if bidx % 10 == 9:\n",
    "            print(f\"SAVING MODEL to {model_path}\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(\"SAVED MODEL\")\n",
    "            print(f\"Epoch: {epoch}, Batch: {bidx}, Loss: {loss.item()}\")\n",
    "            try:\n",
    "                with open(current_params_path, 'w') as f:\n",
    "                    f.write(f\"Epoch: {epoch}, Batch: {bidx}, Loss: {loss.item()}\")\n",
    "            except:\n",
    "                print(\"\\n Could not write to file \\n\")\n",
    "    print(f\"AVG LOSS: {(curr_loss)/len(handwritten_loader)}, Epoch: {epoch+1}\")\n",
    "    prev_loss = curr_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
