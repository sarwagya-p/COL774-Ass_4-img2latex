{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7400e06",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-11-01T16:50:09.066101Z",
     "iopub.status.busy": "2023-11-01T16:50:09.065356Z",
     "iopub.status.idle": "2023-11-01T16:50:11.166235Z",
     "shell.execute_reply": "2023-11-01T16:50:11.164541Z"
    },
    "papermill": {
     "duration": 2.108331,
     "end_time": "2023-11-01T16:50:11.169131",
     "exception": false,
     "start_time": "2023-11-01T16:50:09.060800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "class BLEU(object):\n",
    "    @staticmethod\n",
    "    def compute(candidate, references, weights):\n",
    "        candidate = [c.lower() for c in candidate]\n",
    "        references = [[r.lower() for r in reference] for reference in references]\n",
    "\n",
    "        p_ns = (BLEU.modified_precision(candidate, references, i) for i, _ in enumerate(weights, start=1))\n",
    "        s = math.fsum(w * math.log(p_n) for w, p_n in zip(weights, p_ns) if p_n)\n",
    "\n",
    "        bp = BLEU.brevity_penalty(candidate, references)\n",
    "        return bp * math.exp(s)\n",
    "\n",
    "    @staticmethod\n",
    "    def modified_precision(candidate, references, n):\n",
    "        counts = Counter(ngrams(candidate, n))\n",
    "\n",
    "        if not counts:\n",
    "            return 0\n",
    "\n",
    "        max_counts = {}\n",
    "        for reference in references:\n",
    "            reference_counts = Counter(ngrams(reference, n))\n",
    "            for ngram in counts:\n",
    "                max_counts[ngram] = max(max_counts.get(ngram, 0), reference_counts[ngram])\n",
    "\n",
    "        clipped_counts = dict((ngram, min(count, max_counts[ngram])) for ngram, count in counts.items())\n",
    "\n",
    "        return sum(clipped_counts.values()) / sum(counts.values())\n",
    "    \n",
    "    @staticmethod\n",
    "    def brevity_penalty(candidate, references):\n",
    "        c = len(candidate)\n",
    "        # r = min(abs(len(r) - c) for r in references)\n",
    "        r = min(len(r) for r in references)\n",
    "\n",
    "        if c > r:\n",
    "            return 1\n",
    "        else:\n",
    "            return math.exp(1 - r / c)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02d12eba",
   "metadata": {
    "papermill": {
     "duration": 0.001949,
     "end_time": "2023-11-01T16:50:11.192378",
     "exception": false,
     "start_time": "2023-11-01T16:50:11.190429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADED MODEL to cpu\n",
      "Images shape: torch.Size([2, 3, 224, 224]), formulas shape: torch.Size([2, 405])\n"
     ]
    }
   ],
   "source": [
    "from EncoderDecoder import EncoderDecoder, EncoderCNN, DecoderRNN\n",
    "from data_utils import Img2LatexDataset, load_img\n",
    "from train_model import *\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = load_model(\"./models/model_tf.pt\")\n",
    "model.eval()\n",
    "\n",
    "dataset = Img2LatexDataset(\"../data/SyntheticData/images/\", \"../data/SyntheticData/test.csv\")\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "print(f\"Images shape: {batch[0].shape}, formulas shape: {batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 11, BLEU score: 0.014546339638821704\n",
      "Out of 21, BLEU score: 0.02900341152061404\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Course Files\\Sem 5\\ML\\COL774-Ass_4-img2latex\\submission\\evaluation-metric.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Course%20Files/Sem%205/ML/COL774-Ass_4-img2latex/submission/evaluation-metric.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m overall \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Course%20Files/Sem%205/ML/COL774-Ass_4-img2latex/submission/evaluation-metric.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m counted \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Course%20Files/Sem%205/ML/COL774-Ass_4-img2latex/submission/evaluation-metric.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;49;00m batch \u001b[39min\u001b[39;49;00m loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Course%20Files/Sem%205/ML/COL774-Ass_4-img2latex/submission/evaluation-metric.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     imgs, labels \u001b[39m=\u001b[39;49m batch\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Course%20Files/Sem%205/ML/COL774-Ass_4-img2latex/submission/evaluation-metric.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     preds \u001b[39m=\u001b[39;49m []\n",
      "File \u001b[1;32mc:\\Users\\sarwa\\miniconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\sarwa\\miniconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\sarwa\\miniconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\sarwa\\miniconda3\\envs\\DeepLearning\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Course Files\\Sem 5\\ML\\COL774-Ass_4-img2latex\\submission\\data_utils.py:55\u001b[0m, in \u001b[0;36mImg2LatexDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m---> 55\u001b[0m     img \u001b[39m=\u001b[39m load_img(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimg_dir \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_frame[\u001b[39m\"\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m\"\u001b[39;49m][index], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimg_size)\n\u001b[0;32m     56\u001b[0m     \u001b[39mreturn\u001b[39;00m img, torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_frame[\u001b[39m\"\u001b[39m\u001b[39mIndexList\u001b[39m\u001b[39m\"\u001b[39m][index], requires_grad\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Course Files\\Sem 5\\ML\\COL774-Ass_4-img2latex\\submission\\data_utils.py:16\u001b[0m, in \u001b[0;36mload_img\u001b[1;34m(path, size)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_img\u001b[39m(path, size \u001b[39m=\u001b[39m (\u001b[39m224\u001b[39m, \u001b[39m224\u001b[39m)):\n\u001b[1;32m---> 16\u001b[0m     img \u001b[39m=\u001b[39m (Image\u001b[39m.\u001b[39;49mopen(path))\n\u001b[0;32m     17\u001b[0m     transform \u001b[39m=\u001b[39m transforms\u001b[39m.\u001b[39mCompose([transforms\u001b[39m.\u001b[39mResize(size, antialias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), transforms\u001b[39m.\u001b[39mToTensor(), transforms\u001b[39m.\u001b[39mNormalize(mean\u001b[39m=\u001b[39m[\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m], std\u001b[39m=\u001b[39m[\u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m, \u001b[39m0.5\u001b[39m])])\n\u001b[0;32m     18\u001b[0m     im \u001b[39m=\u001b[39m transform(img)\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\sarwa\\miniconda3\\envs\\DeepLearning\\Lib\\site-packages\\PIL\\Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3215\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3217\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3218\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39mopen(filename, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   3219\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "overall = 0\n",
    "\n",
    "counted = 1\n",
    "\n",
    "for batch in loader:\n",
    "    imgs, labels = batch\n",
    "    preds = []\n",
    "\n",
    "    for i in range(len(labels)):\n",
    "        preds.append(\" \".join(model(imgs[i])))\n",
    "        counted += 1\n",
    "        \n",
    "    ground_truths = [\" \".join([model.decoder.vocab[tok] for tok in labels[i]]) for i in range(len(labels))]\n",
    "    for gt, pred in zip(ground_truths, preds):\n",
    "        gt = gt.split()\n",
    "        pred = pred.split()\n",
    "        overall += BLEU.compute(pred,[gt], weights=[1/4, 1/4, 1/4, 1/4])\n",
    "\n",
    "    if counted % 10 == 1:\n",
    "        print(f\"Out of {counted}, BLEU score: {overall/counted}\")\n",
    "\n",
    "print(\"Macro Bleu : \", overall/counted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ { \\cal L } _ { \\mu } = { \\cal L } _ { \\mu } ^ { \\mu } { \\cal { \\cal } } _ { \\mu } ^ { \\mu } $\n",
      "\\lefteqn\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = batch\n",
    "pred0 = \" \".join(model(imgs[0]))\n",
    "print(pred0)\n",
    "print(model.decoder.vocab_dict[\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.465098,
   "end_time": "2023-11-01T16:50:11.714543",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-01T16:50:05.249445",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
